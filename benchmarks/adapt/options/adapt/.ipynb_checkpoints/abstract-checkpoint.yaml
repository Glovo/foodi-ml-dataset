dataset:
  text_repr: word
  loader_name: indisk_image
  train:
    batch_size: 30
    workers: 1
  val:
    batch_size: 30
    limit: 5000
  adapt:
    data: []
model:
  latent_size: 2048
  #freeze_modules: [model.txt_enc.embed.glove,]
  txt_enc:
    name: gru #gru_glove
    params:
      embed_dim: 300
      use_bi_gru: true
      #add_rand_embed: true
    pooling: none
    devices: [cuda,]
  img_enc:
    name: full_image #simple
    params:
      img_dim: 2048
    devices: [cuda,]
    pooling: none
  similarity:
    name: cosine
    params:
      latent_size: 2048
      gamma: 10
      train_gamma: False
      device: cuda
    device: cuda
optimizer:
  name: adamax
  params:
    lr: 0.001 # 7e-4
    gradual_warmup_steps: [0.5, 2.0, 4000] #torch.linspace
    lr_decay_epochs: [10000, 20000, 3000] #range
    lr_decay_rate: .25
  lr_scheduler: 
    name: null
    params:
      step_size: 1000
      gamma: 1
  grad_clip: 2.
criterion:
  margin: 0.2
  max_violation: False
  beta: 0.991
engine:
  eval_before_training: False  
  print_freq: 25
  nb_epochs: 50
  early_stop: 20
  valid_interval: 235 # before 5, after each epoch
