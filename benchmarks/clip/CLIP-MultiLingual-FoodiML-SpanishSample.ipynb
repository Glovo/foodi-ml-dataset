{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af258234",
   "metadata": {},
   "source": [
    "# Multilingual Joint Image & Text Embeddings \n",
    "\n",
    "This example shows how [SentenceTransformers](https://www.sbert.net) can be used to map images and texts to the same vector space. \n",
    "\n",
    "As model, we use the [OpenAI CLIP Model](https://github.com/openai/CLIP), which was trained on a large set of images and image alt texts.\n",
    "\n",
    "The original CLIP Model only works for English, hence, we used [Multilingual Knowlegde Distillation](https://arxiv.org/abs/2004.09813) to make this model work with 50+ languages.\n",
    "\n",
    "As a source for fotos, we use the [Unsplash Dataset Lite](https://unsplash.com/data), which contains about 25k images. See the [License](https://unsplash.com/license) about the Unsplash images. \n",
    "\n",
    "Note: 25k images is rather small. If you search for really specific terms, the chance are high that no such photo exist in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d395c8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c9c7d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from sentence_transformers) (4.11.3)\n",
      "Requirement already satisfied: tokenizers>=0.10.3 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from sentence_transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from sentence_transformers) (4.61.1)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.10.0-cp36-cp36m-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001B[K     |███████████▊                    | 323.0 MB 143.2 MB/s eta 0:00:04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[K     |███████████████████████▌        | 647.3 MB 113.1 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[K     |████████████████████████████████| 881.9 MB 624 bytes/s ta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from sentence_transformers) (0.6.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from sentence_transformers) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from sentence_transformers) (0.24.2)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from sentence_transformers) (1.5.3)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from sentence_transformers) (3.6.2)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.2 MB 74.4 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: huggingface-hub in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from sentence_transformers) (0.0.19)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.6.0->sentence_transformers) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.0.46)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.4.4)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (4.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.6.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from nltk->sentence_transformers) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from nltk->sentence_transformers) (8.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from scikit-learn->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from torchvision->sentence_transformers) (8.3.2)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.11.1-cp36-cp36m-manylinux1_x86_64.whl (23.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 23.3 MB 59.6 MB/s eta 0:00:01\n",
      "\u001B[?25hBuilding wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=121580 sha256=8fa1233010ca355aadf0159cbfc79744a53cca87b95e8c94fd7a202e1e5926a0\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/4e/6f/20/06e0c1e209742a37ce7a5a9aa4e420a3abd5081c65b4b34d0a\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: torch, torchvision, sentencepiece, sentence-transformers\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.5.1\n",
      "    Uninstalling torch-1.5.1:\n",
      "      Successfully uninstalled torch-1.5.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.6.1\n",
      "    Uninstalling torchvision-0.6.1:\n",
      "      Successfully uninstalled torchvision-0.6.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torcheia 1.0.0 requires torch==1.5.1, but you have torch 1.10.0 which is incompatible.\u001B[0m\n",
      "Successfully installed sentence-transformers-2.1.0 sentencepiece-0.1.96 torch-1.10.0 torchvision-0.11.1\n",
      "\u001B[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3496ac2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
      "\u001B[K     |████████████████████████████████| 64 kB 4.7 MB/s  eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages (from ftfy) (0.2.5)\n",
      "Building wheels for collected packages: ftfy\n",
      "  Building wheel for ftfy (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=42256 sha256=f2be7caff5432f051777e4ce0b2a42cb6902b3c4aab1cee4c177867cafdcba1a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ff/2a/24/75041425faf3347ab146a4a3d0484f723b2c44a7966a06e3f0\n",
      "Successfully built ftfy\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-6.0.3\n",
      "\u001B[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7e258278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image, PngImagePlugin\n",
    "import glob\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPImage\n",
    "import os\n",
    "import tqdm\n",
    "from tqdm import tqdm as tqdm_n\n",
    "import hashlib\n",
    "#from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8955fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c22553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.LOAD_TRUNCATED_IMAGES = True # Otherwise we got ValueError: Decompressed data too large\n",
    "LARGE_ENOUGH_NUMBER = 100\n",
    "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2d9cd",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7078f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "PATH_SGM = '/home/ec2-user/SageMaker/'\n",
    "PATH_DATASET = os.path.join(PATH_SGM, 'anonymized-dataset')\n",
    "PATH_IMGS = os.path.join(PATH_DATASET, 'anonymized_subset')\n",
    "PATH_SAMPLES = os.path.join(PATH_DATASET, 'anonymized_subset.csv')\n",
    "PATH_CLIP = os.path.join(PATH_DATASET, \"benchmarks\", \"clip\")\n",
    "\n",
    "# Path all samples\n",
    "PATH_ALL_SAMPLES = os.path.join(PATH_SGM, 'dataset', 'samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1941b",
   "metadata": {},
   "source": [
    "# Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f43277c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all samples\n",
    "samples_all = pd.read_parquet(PATH_ALL_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "19da1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load valid answers\n",
    "path_valid_answers = os.path.join(PATH_CLIP, \"valid_answers_anonymized_subset.pickle\")\n",
    "with open(path_valid_answers, 'rb') as handle:\n",
    "    valid_answers = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0183edad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 10000\n"
     ]
    }
   ],
   "source": [
    "# (Anonymized SUBSET) Read samples csv\n",
    "samples = pd.read_csv(PATH_SAMPLES)\n",
    "print(\"Images:\", samples.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c15f1",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f06ca4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace s3_path by the sample path\n",
    "replace_s3_path = '/home/ec2-user/SageMaker/dataset/dataset'\n",
    "samples[\"s3_path\"] = samples[\"s3_path\"].str.replace(replace_s3_path, PATH_IMGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce9ccfb",
   "metadata": {},
   "source": [
    "# Valid answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bc8f7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_valid_answers(data: pd.DataFrame):\n",
    "    \"\"\"Generates the valid answers taking into account mutiple images and captions. \n",
    "    For the following dataset we will create the dictionary with valid_answers:\n",
    "    id    caption    hash    |    valid_answers\n",
    "    0     ABC        X       |    0,1,2,4\n",
    "    1     EFG        X       |    0,1,4\n",
    "    2     ABC        Y       |    0,2\n",
    "    3     HIJ        Z       |    3,\n",
    "    4     KLM        X       |    0,1,4\n",
    "    \"\"\"\n",
    "    data[\"cap_hash\"] = data[\"caption\"].apply(lambda x : hashlib.md5(str.encode(x)).hexdigest())\n",
    "    valid_answers = {}\n",
    "\n",
    "    for i, row in tqdm.tqdm(data.iterrows()):\n",
    "        idxs_where_duplication = (data[\"cap_hash\"] == row[\"cap_hash\"]) | (data[\"hash\"] == row[\"hash\"])\n",
    "        list_indexes_duplication = list(np.where(np.array(idxs_where_duplication.to_list()) == True)[0])\n",
    "        valid_answers[row[\"img_id\"]] = list_indexes_duplication\n",
    "    return valid_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d4f4e103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:15, 656.37it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_answers = _compute_valid_answers(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30501f6a",
   "metadata": {},
   "source": [
    "# CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "aee8902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 512\n",
    "batch_size = 1000\n",
    "batch_t2i = 100\n",
    "batch_i2t = 100\n",
    "\n",
    "# Paths of embeddings\n",
    "path_img_emb = os.path.join(PATH_CLIP, 'img_emb.pt')\n",
    "path_txt_emb = os.path.join(PATH_CLIP, 'txt_emb.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dba4a0",
   "metadata": {},
   "source": [
    "## Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22cea9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilingual CLIP model\n",
    "txt_model = SentenceTransformer('clip-ViT-B-32-multilingual-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a8167b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize text embedding matrix\n",
    "txt_inputs = samples[\"caption\"].values\n",
    "txt_emb = torch.zeros((len(txt_inputs),EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63525f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:31<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# Keep populating the matrix in batches\n",
    "for start_index in tqdm.tqdm(range(0, len(txt_inputs), batch_size)):\n",
    "    txt_batch = txt_inputs[start_index:start_index+batch_size]\n",
    "    txt_emb[start_index:start_index+batch_size] = txt_model.encode(txt_batch, batch_size=batch_size, convert_to_tensor=True, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9c67391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving resulting image embeddings\n",
    "path_txt_emb = os.path.join(PATH_CLIP, 'txt_emb.pt')\n",
    "torch.save(txt_emb, path_txt_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517fa3f4",
   "metadata": {},
   "source": [
    "## Image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "46fce14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode image embeddings, for embedding images, we need the non-multilingual CLIP model\n",
    "img_model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9272ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize image embedding matrix\n",
    "imgs_input = np.array(samples[\"s3_path\"])\n",
    "img_emb = torch.zeros((len(imgs_input),EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "befa4e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p36/lib/python3.6/site-packages/PIL/Image.py:974: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n",
      "100%|██████████| 10/10 [06:58<00:00, 41.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# Keep populating the matrix in batches\n",
    "# process 1k images, consuming 5GB of GPU and very low RAM since only 1000 images are opened at each iter\n",
    "\n",
    "for start_index in tqdm.tqdm(range(0, len(imgs_input), batch_size)):\n",
    "    imgs_batch = imgs_input[start_index:start_index+batch_size]\n",
    "    img_list = [Image.open(filepath).convert(\"RGB\") for filepath in imgs_batch]\n",
    "    img_emb[start_index:start_index+batch_size] = img_model.encode(img_list, batch_size=batch_size, convert_to_tensor=True, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9a49f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving resulting image embeddings\n",
    "path_img_emb = os.path.join(PATH_CLIP, 'img_emb.pt')\n",
    "torch.save(img_emb, path_img_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3443d",
   "metadata": {},
   "source": [
    "## I2T and T2I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c97339cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_2_df(hits, index):\n",
    "    \"\"\"\n",
    "    From the list of lists returned by semantic_search, transforms the data into a dataframe\n",
    "    with 3 columns: \n",
    "    - corpus_id: index of the hit over all the embeddings provided in semantic_search\n",
    "    - score: score of that index of the hit with the query embedding\n",
    "    - query_index: index of the sample used as a query. If perfect matching, the highest score of a \n",
    "        given query_index should have the same value for corpus_id and query_index.\n",
    "    \"\"\"\n",
    "    df_hits = pd.DataFrame()\n",
    "    index_hit = index\n",
    "    for hit in hits:\n",
    "        df_hit = pd.DataFrame(hit).sort_values('score', ascending=False)\n",
    "        df_hit[\"query_index\"] = index_hit\n",
    "        df_hits = pd.concat([df_hits, df_hit])\n",
    "        index_hit += 1\n",
    "    return df_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4224aa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_recall(index_hit, df_hits, samples, valid_answers, recalls_dict):\n",
    "    \"\"\"\n",
    "    index_hit: index of the queried sample\n",
    "    df_hits: dataframe resulting from hits_2_df\n",
    "    samples: dataframe in which we can map the index with the image_id\n",
    "    valid_answers: dict mapping each image_id to its corresponding index\n",
    "    recalls_dict: dictionary with primary key the K top hits and, as values, a binary np.array\n",
    "        with the same size as samples.shape[0], 1 means for that index query we could find in the top K\n",
    "        hits that indices in the hits predicted by the semantic_search\n",
    "    \"\"\"\n",
    "    df_hit = df_hits[df_hits['query_index'] == index_hit].copy()\n",
    "    img_id = samples.iloc[index_hit][\"img_id\"].item()\n",
    "    valid_answers_query = valid_answers[img_id]\n",
    "    \n",
    "    # Iterate over each recall dict\n",
    "    for k in recalls_dict:\n",
    "        \n",
    "        # Only get the top K hits\n",
    "        df_k = df_hit.head(k)\n",
    "        \n",
    "        # Get the predicted K top hits sorted by score, and get its corpus_id (index)\n",
    "        predicted_hits_query = df_k['corpus_id'].values\n",
    "        \n",
    "        # See if those indices of the top K hits intersect with the valid answers indeces\n",
    "        intersect_hits_answers = np.intersect1d(valid_answers_query, predicted_hits_query)\n",
    "        \n",
    "        # if they do, update the recalls_dict in that index position for that top K hits\n",
    "        if len(intersect_hits_answers) > 0:\n",
    "            recalls_dict[k][index_hit] = 1\n",
    "    return recalls_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d7aaae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_recalls(k_list, length):\n",
    "    \"\"\"\n",
    "    Initializes the binary arrays for each top K recalls that we want to assess\n",
    "    k_list: list of the top K positions of a given set of ordered hits (i.e [1, 5, 10])\n",
    "    length: number of total queries that we will make, for each query we will have a 0 or 1 in that position \n",
    "        of the array, indicating if we found the query in the top hits (=1) or not (=0)\n",
    "    \"\"\"\n",
    "    r_at_dict = {}\n",
    "    for k in k_list:\n",
    "        r_at_dict[k] = np.zeros(length)\n",
    "    return r_at_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "2061e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(task, recall_dict):\n",
    "    report_dict = {}\n",
    "    for k in recall_dict:\n",
    "        report_dict[k] = 100.0 * np.round((np.sum(recall_dict[k]) / len(recall_dict[k])),4)\n",
    "        print(f\"{task}: Recall at {k}: \", np.round(report_dict[k],2), \"%\")\n",
    "    return report_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072023a",
   "metadata": {},
   "source": [
    "### (Retrieval) Text 2 Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "448cd069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings of images\n",
    "img_emb = torch.load(path_img_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "28bbc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "r_at_t2i = init_recalls([1,5,10], samples.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a15467dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:48<00:00,  2.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over dataset in batch mode\n",
    "for index in tqdm.tqdm(range(0, samples.shape[0], batch_t2i)):\n",
    "    \n",
    "    # Get the rows of the sample batch\n",
    "    sample_query = samples.iloc[index:index + batch_t2i]\n",
    "    \n",
    "    # Get the captions of the batch\n",
    "    query = sample_query['caption'].tolist()\n",
    "    \n",
    "    # Forward it to the model to get the embedding (1x512 torch tensor)\n",
    "    query_emb = txt_model.encode(query, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    # Get the top 10 hits\n",
    "    hits = util.semantic_search(query_emb, img_emb, top_k=10)\n",
    "    \n",
    "    # Get it as a dataframe, adding as column the index of the iteration\n",
    "    df_hits = hits_2_df(hits, index)\n",
    "    \n",
    "    for index_hit in list(range(index,index + batch_t2i)):\n",
    "        r_at_t2i = update_recall(index_hit, df_hits, samples, valid_answers, r_at_t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4cc82dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2I: Recall at 1:  6.05 %\n",
      "T2I: Recall at 5:  16.42 %\n",
      "T2I: Recall at 10:  23.73 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 6.05, 5: 16.42, 10: 23.73}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(\"T2I\", r_at_t2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c53f6d",
   "metadata": {},
   "source": [
    "### (Annotation) Image 2 Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "595c6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings of captions\n",
    "txt_emb = torch.load(path_txt_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5ad34969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "r_at_i2t = init_recalls([1,5,10], samples.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ba8100c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:06<00:00,  3.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over dataset in batch mode\n",
    "for index in tqdm.tqdm(range(0, samples.shape[0], batch_i2t)):\n",
    "    \n",
    "    # Get the rows of the sample batch\n",
    "    sample_query = samples.iloc[index:index + batch_i2t]\n",
    "    batch_img_paths = sample_query['s3_path'].tolist()\n",
    "    \n",
    "    # Get the images of the batch\n",
    "    query = [Image.open(filepath).convert(\"RGB\") for filepath in batch_img_paths]\n",
    "    \n",
    "    # Forward it to the model to get the embedding (1x512 torch tensor)\n",
    "    query_emb = img_model.encode(query, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    # Get the top 10 hits\n",
    "    hits = util.semantic_search(query_emb, txt_emb, top_k=10)\n",
    "    \n",
    "    # Get it as a dataframe, adding as column the index of the iteration\n",
    "    df_hits = hits_2_df(hits, index)\n",
    "    \n",
    "    for index_hit in list(range(index,index + batch_i2t)):\n",
    "        r_at_i2t = update_recall(index_hit, df_hits, samples, valid_answers, r_at_i2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a63cc53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I2T: Recall at 1:  5.42 %\n",
      "I2T: Recall at 5:  16.63 %\n",
      "I2T: Recall at 10:  22.19 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 5.42, 5: 16.63, 10: 22.189999999999998}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(\"I2T\", r_at_i2t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbede67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p36",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}